{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mutagen\n",
    "import os\n",
    "import random\n",
    "import wave\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import classifier\n",
    "import multi_classifier as mclass\n",
    "import irmasTrainUtils as trainUtils\n",
    "import irmasTestUtils as testUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: 1\n",
      "Processing directory: 2\n",
      "Processing directory: 3\n",
      "Processing directory: 4\n",
      "Processing directory: 5\n",
      "Processing directory: 6\n",
      "Processing directory: 7\n",
      "Processing directory: 8\n",
      "Processing directory: 9\n",
      "Processing directory: 10\n",
      "Processing directory: 11\n",
      "Processing directory: 12\n"
     ]
    }
   ],
   "source": [
    "trainUtils.parse_irmas_trainset(\"IRMAS-TrainingData\", \"Preprocessed_Trainset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  0\n",
      "Count:  500\n",
      "Count:  1000\n",
      "Count:  1500\n",
      "Count:  2000\n",
      "Count:  2500\n",
      "Count:  3000\n",
      "Count:  3500\n",
      "Count:  4000\n",
      "Count:  4500\n",
      "Count:  5000\n",
      "Count:  5500\n",
      "Count:  6000\n",
      "Count:  6500\n",
      "Count:  7000\n",
      "Count:  7500\n",
      "Count:  8000\n",
      "Count:  8500\n",
      "Count:  9000\n",
      "Count:  9500\n",
      "Count:  10000\n",
      "Count:  10500\n",
      "Count:  11000\n",
      "Count:  11500\n",
      "Count:  12000\n",
      "Count:  0\n",
      "Count:  500\n",
      "Count:  1000\n"
     ]
    }
   ],
   "source": [
    "df_train = trainUtils.load_train_dataset(\"Preprocessed_Trainset/Train\")\n",
    "df_valid = trainUtils.load_train_dataset(\"Preprocessed_Trainset/Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(\"Preprocessed_Trainset/pickled_train.pkl\")\n",
    "df_valid.to_pickle(\"Preprocessed_Trainset/pickled_validation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = utils.load_pickled_dataset(\"Preprocessed_Trainset/pickled_train.pkl\")\n",
    "df_valid = utils.load_pickled_dataset(\"Preprocessed_Trainset/pickled_validation.pkl\")\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_valid = df_valid.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "377/377 [==============================] - 101s 255ms/step - loss: 2.5355\n",
      "Epoch 2/30\n",
      "377/377 [==============================] - 97s 258ms/step - loss: 1.4609\n",
      "Epoch 3/30\n",
      "377/377 [==============================] - 99s 261ms/step - loss: 1.2059\n",
      "Epoch 4/30\n",
      "377/377 [==============================] - 93s 247ms/step - loss: 0.9147\n",
      "Epoch 5/30\n",
      "377/377 [==============================] - 93s 246ms/step - loss: 0.6876\n",
      "Epoch 6/30\n",
      "377/377 [==============================] - 93s 247ms/step - loss: 0.4955\n",
      "Epoch 7/30\n",
      "377/377 [==============================] - 93s 247ms/step - loss: 0.4039\n",
      "Epoch 8/30\n",
      "377/377 [==============================] - 89s 235ms/step - loss: 0.2399\n",
      "Epoch 9/30\n",
      "377/377 [==============================] - 88s 232ms/step - loss: 0.1418\n",
      "Epoch 10/30\n",
      "377/377 [==============================] - 92s 244ms/step - loss: 0.1064\n",
      "Epoch 11/30\n",
      "377/377 [==============================] - 91s 242ms/step - loss: 0.2515\n",
      "Epoch 12/30\n",
      "377/377 [==============================] - 93s 246ms/step - loss: 0.0559\n",
      "Epoch 13/30\n",
      "377/377 [==============================] - 108s 286ms/step - loss: 0.0834\n",
      "Epoch 14/30\n",
      "377/377 [==============================] - 109s 290ms/step - loss: 0.0803\n",
      "Epoch 15/30\n",
      "377/377 [==============================] - 93s 246ms/step - loss: 0.0869\n",
      "Epoch 16/30\n",
      "377/377 [==============================] - 93s 247ms/step - loss: 0.1517\n",
      "Epoch 17/30\n",
      "377/377 [==============================] - 89s 235ms/step - loss: 0.0418\n",
      "Epoch 18/30\n",
      "377/377 [==============================] - 86s 228ms/step - loss: 0.0170\n",
      "Epoch 19/30\n",
      "377/377 [==============================] - 96s 254ms/step - loss: 0.0354\n",
      "Epoch 20/30\n",
      "377/377 [==============================] - 90s 238ms/step - loss: 0.0857\n",
      "Epoch 21/30\n",
      "377/377 [==============================] - 86s 229ms/step - loss: 0.0214\n",
      "Epoch 22/30\n",
      "377/377 [==============================] - 95s 251ms/step - loss: 0.0452\n",
      "Epoch 23/30\n",
      "377/377 [==============================] - 94s 248ms/step - loss: 0.1355\n",
      "Epoch 24/30\n",
      "377/377 [==============================] - 96s 254ms/step - loss: 0.0221\n",
      "Epoch 25/30\n",
      "377/377 [==============================] - 96s 253ms/step - loss: 0.0419\n",
      "Epoch 26/30\n",
      "377/377 [==============================] - 94s 250ms/step - loss: 0.0105\n",
      "Epoch 27/30\n",
      "377/377 [==============================] - 94s 250ms/step - loss: 0.1675\n",
      "Epoch 28/30\n",
      "377/377 [==============================] - 96s 255ms/step - loss: 0.3712\n",
      "Epoch 29/30\n",
      "377/377 [==============================] - 96s 255ms/step - loss: 0.0102\n",
      "Epoch 30/30\n",
      "377/377 [==============================] - 95s 251ms/step - loss: 0.0111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c59054c40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train_model(model, df_train, batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988366295496094\n"
     ]
    }
   ],
   "source": [
    "print(classifier.get_accuracy(model, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5879360465116279\n"
     ]
    }
   ],
   "source": [
    "print(classifier.get_accuracy(model, df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model.weights)\n",
    "model.save_weights(\"classifier_weights_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = classifier.make_model()\n",
    "test.load_weights(\"classifier_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "smaller = mclass.make_model()\n",
    "for layer in test.layers:\n",
    "    for i in range(len(smaller)):\n",
    "        for v in range(len(smaller[i].layers)):\n",
    "            if smaller[i].layers[v].name == layer.name:\n",
    "                smaller[i].layers[v].set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smaller[0].layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, ys = mclass.process_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclass.train_model(smaller, x, ys, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=4, strides=(2,2), padding='same', input_shape=(128,259,1), name=\"conv_1\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=1, name=\"bn_1\"))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), name=\"mpool_1\"))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=(1,1), padding='valid', name=\"conv_2\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=1, name=\"bn_2\"))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), name=\"mpool_2\"))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=(1,1), padding='valid', name=\"conv_3\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=1, name=\"bn_3\"))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), name=\"mpool_3\"))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(500, activation='relu', name=\"fc_4\"))\n",
    "model.add(tf.keras.layers.Dense(11, activation=None, name=\"fc_5\"))\n",
    "model.add(tf.keras.layers.Softmax())\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "\n",
    "train_set = df.tail(-1000)\n",
    "test_set = df.head(1000)\n",
    "\n",
    "x, y = np.stack(train_set.data), np.stack(train_set.label)\n",
    "x = x.reshape((-1,128,259,1))\n",
    "print(x.shape)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "model.fit(x, y, batch_size=32, epochs=30)\n",
    "\n",
    "'''x, y = np.stack(sample['data']), np.stack(sample['label'])\n",
    "print(x.shape, y.shape)\n",
    "print(type(x), type(y))\n",
    "x, y = tf.convert_to_tensor(x, dtype=tf.float32), tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "model.predict(x)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.stack(train_set.data), np.stack(train_set.label)\n",
    "x = x.reshape((-1,128,259,1))\n",
    "\n",
    "preds = np.argmax(model.predict(x), axis=1).reshape(x.shape[0], 1)\n",
    "true = np.argmax(y, axis=1)\n",
    "\n",
    "print(\"Train accuracy:\", np.sum((preds == true)*np.ones(preds.shape))/x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = np.stack(test_set.data), np.stack(test_set.label)\n",
    "x = x.reshape((-1,128,259,1))\n",
    "\n",
    "preds = np.argmax(model.predict(x), axis=1).reshape(x.shape[0], 1)\n",
    "true = np.argmax(y, axis=1)\n",
    "\n",
    "print(\"Test accuracy:\", np.sum((preds == true)*np.ones(preds.shape))/x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 epochs - train 88.0 test 51.5\n",
    "\n",
    "\n",
    "#30 epochs - train accuracy 96.7%, test accuracy 36.8%\n",
    "#20 epochs - train accuracy 98.8%, test accuracy 47.8%\n",
    "#15 epochs - train accuracy 97.5%, test accuracy 36.4%\n",
    "#10 epochs - train accuracy 95.7%, test accuracy 44.4%\n",
    "\n",
    "arr = np.array([[5, 2], [3, 4], [6, 8]])\n",
    "print (arr)\n",
    "print (np.linalg.norm(arr))\n",
    "print (arr / np.linalg.norm(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.combine_dataset(df, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
