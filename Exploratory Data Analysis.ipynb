{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mutagen\n",
    "import os\n",
    "import random\n",
    "import wave\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import classifier\n",
    "import multi_classifier as mclass\n",
    "import irmasTrainUtils as trainUtils\n",
    "import irmasTestUtils as testUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainUtils.parse_irmas_trainset(\"IRMAS-TrainingData\", \"Preprocessed_Trainset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = trainUtils.load_train_dataset(\"Preprocessed_Trainset/Train\")\n",
    "#df_valid = trainUtils.load_train_dataset(\"Preprocessed_Trainset/Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_pickle(\"Preprocessed_Trainset/pickled_train.pkl\")\n",
    "#df_valid.to_pickle(\"Preprocessed_Trainset/pickled_validation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = utils.load_pickled_dataset(\"Preprocessed_Trainset/train.pkl\")\n",
    "df_valid = utils.load_pickled_dataset(\"Preprocessed_Trainset/validation.pkl\")\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_valid = df_valid.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.train_model(model, df_train, batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1faefef4c40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"Saved_Weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "0.7036668325866932\n"
     ]
    }
   ],
   "source": [
    "print(classifier.get_accuracy(model, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "0.6047197640117994\n"
     ]
    }
   ],
   "source": [
    "print(classifier.get_accuracy(model, df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = mclass.make_model()\n",
    "mclass.load_weights(multi_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "377/377 [==============================] - 86s 213ms/step - loss: 2.1861\n",
      "Epoch 2/30\n",
      "377/377 [==============================] - 98s 260ms/step - loss: 1.4853\n",
      "Epoch 3/30\n",
      "377/377 [==============================] - 100s 265ms/step - loss: 1.3472\n",
      "Epoch 4/30\n",
      "377/377 [==============================] - 90s 240ms/step - loss: 1.2451\n",
      "Epoch 5/30\n",
      "377/377 [==============================] - 92s 243ms/step - loss: 1.2276\n",
      "Epoch 6/30\n",
      "377/377 [==============================] - 78s 207ms/step - loss: 1.1741\n",
      "Epoch 7/30\n",
      "377/377 [==============================] - 81s 215ms/step - loss: 1.1497\n",
      "Epoch 8/30\n",
      "377/377 [==============================] - 83s 221ms/step - loss: 1.1144\n",
      "Epoch 9/30\n",
      "377/377 [==============================] - 77s 204ms/step - loss: 1.1165\n",
      "Epoch 10/30\n",
      "377/377 [==============================] - 78s 207ms/step - loss: 1.0949\n",
      "Epoch 11/30\n",
      "377/377 [==============================] - 83s 219ms/step - loss: 1.0679\n",
      "Epoch 12/30\n",
      "377/377 [==============================] - 80s 212ms/step - loss: 1.0915\n",
      "Epoch 13/30\n",
      "377/377 [==============================] - 76s 202ms/step - loss: 1.0419\n",
      "Epoch 14/30\n",
      "377/377 [==============================] - 84s 224ms/step - loss: 1.0101\n",
      "Epoch 15/30\n",
      "377/377 [==============================] - 82s 216ms/step - loss: 1.0256\n",
      "Epoch 16/30\n",
      "377/377 [==============================] - 76s 203ms/step - loss: 1.0264\n",
      "Epoch 17/30\n",
      "377/377 [==============================] - 83s 219ms/step - loss: 1.0050\n",
      "Epoch 18/30\n",
      "377/377 [==============================] - 83s 220ms/step - loss: 0.9804\n",
      "Epoch 19/30\n",
      "377/377 [==============================] - 82s 218ms/step - loss: 0.9906\n",
      "Epoch 20/30\n",
      "377/377 [==============================] - 80s 212ms/step - loss: 0.9393\n",
      "Epoch 21/30\n",
      "377/377 [==============================] - 81s 216ms/step - loss: 0.9809\n",
      "Epoch 22/30\n",
      "377/377 [==============================] - 80s 212ms/step - loss: 0.9506\n",
      "Epoch 23/30\n",
      "377/377 [==============================] - 82s 219ms/step - loss: 0.9438\n",
      "Epoch 24/30\n",
      "377/377 [==============================] - 81s 214ms/step - loss: 0.9703\n",
      "Epoch 25/30\n",
      "377/377 [==============================] - 76s 201ms/step - loss: 0.9366\n",
      "Epoch 26/30\n",
      "377/377 [==============================] - 84s 223ms/step - loss: 0.9349\n",
      "Epoch 27/30\n",
      "377/377 [==============================] - 76s 201ms/step - loss: 0.9551\n",
      "Epoch 28/30\n",
      "377/377 [==============================] - 85s 224ms/step - loss: 0.8931\n",
      "Epoch 29/30\n",
      "377/377 [==============================] - 82s 219ms/step - loss: 0.9335\n",
      "Epoch 30/30\n",
      "377/377 [==============================] - 76s 203ms/step - loss: 0.9069\n"
     ]
    }
   ],
   "source": [
    "mclass.train_model(multi_model, df_train, batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0909090909090909\n"
     ]
    }
   ],
   "source": [
    "print(mclass.get_accuracy(multi_model, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09090909090909094\n"
     ]
    }
   ],
   "source": [
    "print(mclass.get_accuracy(multi_model, df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86317337 0.36226666 0.0948779  0.14307916 0.30608308 0.06918737\n",
      " 0.21820486 0.82087505 0.53484875 0.88276625 0.01191196]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, y = np.stack(df_train.data), np.stack(df_train.label)\n",
    "x = x.reshape((-1,128,259,2))\n",
    "print(multi_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "smaller = mclass.make_model()\n",
    "for layer in test.layers:\n",
    "    for i in range(len(smaller)):\n",
    "        for v in range(len(smaller[i].layers)):\n",
    "            if smaller[i].layers[v].name == layer.name:\n",
    "                smaller[i].layers[v].set_weights(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smaller[0].layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, ys = mclass.process_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclass.train_model(smaller, x, ys, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=4, strides=(2,2), padding='same', input_shape=(128,259,1), name=\"conv_1\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=1, name=\"bn_1\"))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), name=\"mpool_1\"))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=(1,1), padding='valid', name=\"conv_2\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=1, name=\"bn_2\"))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), name=\"mpool_2\"))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=(1,1), padding='valid', name=\"conv_3\"))\n",
    "model.add(tf.keras.layers.BatchNormalization(axis=1, name=\"bn_3\"))\n",
    "model.add(tf.keras.layers.ReLU())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), name=\"mpool_3\"))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(500, activation='relu', name=\"fc_4\"))\n",
    "model.add(tf.keras.layers.Dense(11, activation=None, name=\"fc_5\"))\n",
    "model.add(tf.keras.layers.Softmax())\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "\n",
    "train_set = df.tail(-1000)\n",
    "test_set = df.head(1000)\n",
    "\n",
    "x, y = np.stack(train_set.data), np.stack(train_set.label)\n",
    "x = x.reshape((-1,128,259,1))\n",
    "print(x.shape)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "model.fit(x, y, batch_size=32, epochs=30)\n",
    "\n",
    "'''x, y = np.stack(sample['data']), np.stack(sample['label'])\n",
    "print(x.shape, y.shape)\n",
    "print(type(x), type(y))\n",
    "x, y = tf.convert_to_tensor(x, dtype=tf.float32), tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "model.predict(x)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.stack(train_set.data), np.stack(train_set.label)\n",
    "x = x.reshape((-1,128,259,1))\n",
    "\n",
    "preds = np.argmax(model.predict(x), axis=1).reshape(x.shape[0], 1)\n",
    "true = np.argmax(y, axis=1)\n",
    "\n",
    "print(\"Train accuracy:\", np.sum((preds == true)*np.ones(preds.shape))/x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = np.stack(test_set.data), np.stack(test_set.label)\n",
    "x = x.reshape((-1,128,259,1))\n",
    "\n",
    "preds = np.argmax(model.predict(x), axis=1).reshape(x.shape[0], 1)\n",
    "true = np.argmax(y, axis=1)\n",
    "\n",
    "print(\"Test accuracy:\", np.sum((preds == true)*np.ones(preds.shape))/x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 epochs - train 88.0 test 51.5\n",
    "\n",
    "\n",
    "#30 epochs - train accuracy 96.7%, test accuracy 36.8%\n",
    "#20 epochs - train accuracy 98.8%, test accuracy 47.8%\n",
    "#15 epochs - train accuracy 97.5%, test accuracy 36.4%\n",
    "#10 epochs - train accuracy 95.7%, test accuracy 44.4%\n",
    "\n",
    "arr = np.array([[5, 2], [3, 4], [6, 8]])\n",
    "print (arr)\n",
    "print (np.linalg.norm(arr))\n",
    "print (arr / np.linalg.norm(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.combine_dataset(df, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
